---
typora-copy-images-to: ../content
---

# 데이터 전처리

# 1. 라이브러리 불러오기
가장 중요하고 자주 쓰는 3개의 라이브러리

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
```

# 2. 데이터 불러오기
데이터 불러오기
data.csv를 열어보면 이렇게 생겼다.

![image-20191221172525139](/Users/jw/Library/Application Support/typora-user-images/image-20191221172525139.png)

```python
# 데이터 불러오기
dataset = pd.read_csv('Data.csv')

X = dataset.iloc[:, :-1].values
Y = dataset.iloc[:, 3].values
```



iloc 는 [row, column] 이렇게 데이터를 불러오는건데

`X = dataset.iloc[:, :-1].values` 는 모든 row와 처음부터 -1번째 column까지 불러오라는 뜻이다. 즉 마지막 column빼고 다 들고 오라는 뜻

# 빈 데이터 처리하기

데이터 전처리가 중요한데 데이터를 보면 빈 데이터가 2개 있다.

어떻게 해야할까? 삭제 하는게 나을까? 하지만 중요한 데이터라면 삭제하면 안되는 경우가 있다. 이럴 때 사용하는 것이 sklearn의 Imputer 이다.

```python
from sklearn.preprocessing import Imputer

```



```python
imputer = Imputer(missing_vales = 'NaN', strategy = 'mean', axis = 0)
```

## `Imputer` 설명

`Imputer` 를 누르고 ` command I`를 누르면 라이브러리에 대한 설명이 나온다.

아래는 파라미터 설명이다.

### missing_values

missing_values**integer or "NaN", optional (default="NaN")

없는 데이터를 고른다. 주로 없는 데이터는 NaN으로 나오기 떄문에 이게 기본 값이다.

### strategy 

빈 값을 어떤걸로 채울지 알려주는 거다. mean을 하면 평균이 나오는거고,

중위값, 최빈값으로 설정 가능하다.

- If "mean", then replace missing values using the mean along the axis.
- If "median", then replace missing values using the median along the axis.
- If "most_frequent", then replace missing using the most frequent value along the axis.

### axis

어떤 축을 기준으로 설정할지 알려주는 것이다.

0이면 column을 기준으로, 1이면 row를 기준으로 바꾼다.



The axis along which to impute.

- If axis=0, then impute along columns.
- If axis=1, then impute along rows.



```python
from sklearn.preprocessing import Imputer
imputer = Imputer(missing_vales = 'NaN', strategy = 'mean', axis = 0) #1
imputer = imputer.fit(X[:,1:3]) #2
X[:, 1:3] = imputer.transform(X[:, 1:3]) #3
```

#1 을 통해서 데이터를 바꾸고, #2를 통해서 데이터를 변수 X의 데이터 형태로 맞추고, #3 을 통해서 데이터를 덮어 씌운다.



# Categorical Data

수학식에는 문자가 들어갈 수 없지만, 실제 데이터에는 문자가 존재한다. 따라서 `Encode` 라는 작업을 통해서 데이터를 숫자로 바꿔줘야한다.

```python
from sklearn.preprocessing import LabelEncoder
labelencoder_X = LabelEncoder()
labelencoder_X.fit_transform(X[:, 0])
# 실행 결과
# array([0, 2, 1, 2, 1, 0, 2, 0, 1, 0])
X[:, 0] = labelencoder_X.fit_transform(X[:, 0])

```

1. LabelEncoder 라이브러리를 import 해준다.

2. Labelencder_X 변수에 LabelEncoder 객체를 만들어 준다.(메소드를 사용하기 위해서)

3. `label encoder_X.fit_transform(X[:, 0])` 데이터를 보면 0번째 column이 Country라서 이걸 숫자로 바꿀 것이다.

   ![image-20191222135337464](/Users/jw/Desktop/study/machineLearning/content/image-20191222135337464.png)

4. 코드를 실행하면 `array([0, 2, 1, 2, 1, 0, 2, 0, 1, 0])` 이 잘 출력된다.

5. 이제 결과를 다시 X에 넣어준다.

### Encoding이 가지는 문제가 있다.

지금 텍스트를 바꾸는데 성공했지만, 실제 데이터는 독일, 스페인, 프랑스 3개의 한 카테고리에 묶인 상하 개념이 없는 데이터지만, 숫자료 encoding 한 이후에는 머신러닝 모델의 경우 0이 1보다 크고, 1이 2보다 크다고 생각하기 때문에 이건 정확하지 않은 결과를 만들 수 있다. 따라서 머신러닝 모델이 변경된 숫자 데이터를 하나의 다른 숫자로 바꾸는 작업을 해야한다. 이럴 때 사용하는 것이 dummy variables이다.

이런 것들을 막기 위해서 이제 column의 category 숫자만큼 column을 만든다. 

![image-20191222140918122](/Users/jw/Desktop/study/machineLearning/content/image-20191222140918122.png)

이제 코드를 작성해보자.



```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X = LabelEncoder()
labelencoder_X.fit_transform(X[:, 0])
# 실행 결과
# array([0, 2, 1, 2, 1, 0, 2, 0, 1, 0])
X[:, 0] = labelencoder_X.fit_transform(X[:, 0])
# 어떤 줄을 기준으로 분류할 지 정해준다.
onehotencoder = OneHotEncoder(categorical_features = [0])
# 데이터 형태로 만들어준다.
X = onehotencoder.fit_transform(X).toarray()
```



1. 기존의 코드에 `OneHotEncoder` 라이브러리를 불러오자.
2. `onehotencoder = OneHotEncoder(categorical_features = [0])` 
   카테고리를 몇번째 column 으로 정할지 정해준다.

3. X = onehotencoder.fit_transform(X).toarray() X에 데이터를 넣어준다.

![image-20191222144434768](/Users/jw/Desktop/study/machineLearning/content/image-20191222144434768.png)

기존 데이터와 비교해보면 Country가 3개의 column으로 변경되었다.

1번째는 France 2번째는 Germany 3번째는 Spain이다. 그다음에는 age와 Salary가 들어가 있다.

### Purchased column encoding 하기

Purchased는 2개의 항목 no, yes만 있기 때문에 자동적으로 머신러닝 모델이 데이터간 상하관계가 없다는 것을 깨닫는다. 따라서 LabelEncoder만 해주겠다.

```python
labelencoder_Y = LabelEncoder()
Y = labelencoder_Y.fit_transform(Y)
```



### Updates

문법 변화가 있음 참고용!

in the following tutorial, the first line of code we will type will be:

```
from sklearn.cross_validation import train_test_split
```

However the "cross_validation" name is now deprecated and was replaced by "model_selection" inside the new anaconda versions.

Therefore you might get a warning or even an error if you run this line of code above.

To avoid this, you just need to replace:

```
from sklearn.cross_validation import train_test_split
```

by

```
from sklearn.model_selection import train_test_split
```

and you will get no warning ;)

Hope you are having a great ML journey so far, I'll see you in the next tutorial.



Updates:

***** Also, for those of you who run into Sklearn related deprecations regarding the imputer class you can use the following modifications:

 sklearn.preprocessing.Imputer 클래스는 사이킷런 0.20 버전에서 사용 중지 경고가 발생합니다. 이 클래스는 0.22 버전에서 삭제될 예정입니다. 그 대신 sklearn.impute.SimpleImputer 클래스가 추가되었습니다. SimpleImputer 클래스는 Imputer 클래스와 사용법이 비슷합니다. SimpleImputer를 사용하여 본문처럼 평균을 대체하는 코드는 다음과 같습니다.

OLD (NO)

```python
from sklearn.preprocessing import Imputer
imputer = Imputer(missing_vales = 'NaN', strategy = 'mean', axis = 0) #1
imputer = imputer.fit(X[:,1:3]) #2
X[:, 1:3] = imputer.transform(X[:, 1:3]) #3
```

NEW (YES)

```python
from sklearn.impute import SimpleImputer
missingvalues = SimpleImputer(missing_values = np.nan, strategy = 'mean', verbose = 0)
missingvalues = missingvalues.fit(X[:, 1:3])
X[:, 1:3]=missingvalues.transform(X[:, 1:3])

```



# Splitting the Dataset into the Training set and Test set

데이터가 있을 때 머신러닝을 위해서는 training set 과 test set 2개로 나눠야 한다. 머신 러닝이란 머신들이 데이터를 통해서 모델을 만들면서 학습한다는 것이기 때문에 데이터를 나누는게 중요하다.



## Splitting the dataset into the Training set and Test set

머신러닝 모델을 학습하고 그 결과를 검증하기 위해서는 원래의 데이터를 Training, Validation, Testing의 용도로 나누어 다뤄야 한다. 그렇지 않고 Training에 사용한 데이터를 검증용으로 사용하면 시험문제를 알고 있는 상태에서 공부를 하고 그 지식을 바탕으로 시험을 치루는 꼴이므로 제대로 된 검증이 이루어지지 않기 때문이다. 



딥러닝을 제외하고도 다양한 기계학습과 데이터 분석 툴을 제공하는 scikit-learn 패키지 중 model_selection에는 데이터 분할을 위한 train_test_split 함수가 들어있다.

```python
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
```

`train_test_split()` 의 옵션 설명

**arrays** : 분할시킬 데이터를 입력 *(Python list, Numpy array, Pandas dataframe 등..)*

**test_size** : 테스트 데이터셋의 비율(float)이나 갯수(int) *(default = 0.25)*

**train_size** : 학습 데이터셋의 비율(float)이나 갯수(int) *(default = test_size의 나머지)*

보통 test_size를 정해주면 나머지는 train_size로 정해짐

**random_state** : 데이터 분할시 셔플이 이루어지는데 이를 위한 시드값 *(int나 RandomState로 입력)*

**shuffle** : 셔플여부설정 *(default = True)*

**stratify** : 지정한 Data의 비율을 유지한다. 예를 들어, Label Set인 Y가 25%의 0과 75%의 1로 이루어진 Binary Set일 때, stratify=Y로 설정하면 나누어진 데이터셋들도 0과 1을 각각 25%, 75%로 유지한 채 분할된다.



코드를 실행하면 아래처럼 테스트 데이터에는 2개, 트레인 데이터 8개의 데이터가 들어간다.

![image-20191222164459606](/Users/jw/Desktop/study/machineLearning/content/image-20191222164459606.png)



위 traindata를 보고 훈련을 하고, test 데이터를 보고 모델이 잘 작동하는지 확인한다.

![image-20191222165949551](/Users/jw/Desktop/study/machineLearning/content/image-20191222165949551.png)

테스트에서는 0번 사람이 0번 결과(안산다) 나오고, 1번 사람도 안산다가 나와야 함. 하지만 머신러닝 모델이 hard correlations로 학습했기 때문에 (강한 상관관계?) 모델이 상관관계 파악을 잘 못했을 것 같다. 따라서 좋은 모델을 못만들게 되는데 이게 overfitting이다.

나중에 배운다.

## Feature Scaling

Feature Scaling이란? 

데이터를 보면 이름은 27~50살의 반경을 가지고 있고, 연봉(salary)는 48000~ 83000까지 반경을 가지고 있다.

이러한 범위 격차는 머신러닝 모델에 문제를 일으킬 수 있다.

![image-20191222171115289](/Users/jw/Desktop/study/machineLearning/content/image-20191222171115289.png)



왜냐하면 변수간의 변화를 Euclidean Distance 공식을 이용하는데, 이렇게 되는 경우 salary는 훨씬 큰 값을 갖게 될 것이다.

만약 2번째 Spain과 7번째 France는 salary 차이는 31000 961000000이다

나이 차이는 21 -> 441 이다.

너무 큰 격차가 있기 때문에 같은 범위로 만들어서 -1~1 까지 범위로 변경해야 한다.

![image-20191222171357775](/Users/jw/Desktop/study/machineLearning/content/image-20191222171357775.png)

아래는 정규화 식이다. 수학에 대해서 많이 알 필요는 없지만 기본적인건 알아 두는 것이 좋을 것 같다.

![image-20191222171732312](/Users/jw/Desktop/study/machineLearning/content/image-20191222171732312.png)

### Feature Scaling

```python
# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
```



코드를 실행하면 이런식으로 Feature Scaling이 잘 되었다.

![image-20191222172812653](/Users/jw/Desktop/study/machineLearning/content/image-20191222172812653.png)



## Feature Scaling에 대한 질문

1. dummy variables도 Feature Scaling을 해야하나요? 

   어떤 사람은 필요가 없다고 하고, 어떤 사람은 필요하다고 하는데, 개인적으로는 상황에 따라 다를 것 같다.

2. Feature Scaling을 Y에도 적용해야하나요?

   이번 경우에는 안해도 됩니다. 만약에 데이터가 엄청나게 차이나는 경우 해야겠지만 지금은 0,1 이렇게 나눠져 있기 때문에 안해도 된다.



# Regression

Regression models (both linear and non-linear) are used for predicting a real value, like salary for example. If your independent variable is time, then you are forecasting future values, otherwise your model is predicting present but unknown values. Regression technique vary from Linear Regression to SVR and Random Forests Regression.

In this part, you will understand and learn how to implement the following Machine Learning Regression models:

1. Simple Linear Regression
2. Multiple Linear Regression
3. Polynomial Regression
4. Support Vector for Regression (SVR)
5. Decision Tree Classification
6. Random Forest Classification



### 데이터 살펴보기

데이터를 살펴보면 연봉과 근무연수가 있는데 둘의 관계를 파악하는 것이다.

![image-20191225172801380](/Users/jw/Desktop/study/machineLearning/content/image-20191225172801380.png)



가장 기본적인 simple Linear Regression은 

y = b0 + b1 * x1 이다

y는 Dependent variable (DV)

x는 Independent varable(IV)

b1은 Coeffcient x1의 변화량의 기울기

![image-20191225173656022](/Users/jw/Desktop/study/machineLearning/content/image-20191225173656022.png)

Simple Linear Regression!

그러면 어떻게 이런 best fitting line or Simple Linear Regression 얻는가?

선을 엄청 많이 그어서 가장 차이가 적은 것을 찾는다. 이것이 best fitting line or ordinary least squares method라고 한다.

## 데이터 준비하기



```python
# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Salary_Data.csv')
X = dataset.iloc[:, : -1].values
# 연봉이 1번 index에 있어서 아래와 같이 데이터를 넣음
y = dataset.iloc[:, 1].values

# Splitting the dataset into the Training set and Test set
# 모델이 30개라서 테스트에 10개, train에 20개 넣는다.
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state = 0)
```

테스트 데이터와, train 데이터를 분리하였다. 데이터가 30개여서 10개는 테스트, 20개는 훈련으로 만들었다.

## 훈련 데이터에 fitting Simple Linear Regression 적용시키기

LinearRegression 라이브러리를 사용하여 simple Linear Regression 모델을 만드는 것이다. 보통 regressor라는 변수명을 사용한다.

```python
# Fitting Simple Linear Regression to the Training set
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)

```

모델을 이용하여 Test set의 결과가 정확했는지 확인할 수 있다.

```python
# Predicting the Test set results
y_pred = regressor.predict(X_test)
```

![image-20191225190545334](/Users/jw/Desktop/study/machineLearning/content/image-20191225190545334.png)

왼쪽이 진짜고 오른쪽이 모델을 통해 예측한 예측 값이다. 9번의 경우 예측이 잘 되었고, 7번의 경우 잘 안되었다.



### simple Linear Regression 시각화하기

```python
# Visualising the Training set results
plt.scatter(X_train, y_train, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Salary vs Experience (Training set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()
```

빨간색으로 근속연수와, 연봉을 점 그래프로 나타내고, 선 그래프로 예상 연봉을 만들었다.

![image-20191225191327154](/Users/jw/Desktop/study/machineLearning/content/image-20191225191327154.png)

결과를 보면 데이터들이 선형적으로 잘 나타나있는지 알 수 있다.

그렇다면 과연 새로운 빨간 점(데이터)를 넣으면 어떻게 될까?

테스트 데이터를 넣어서 모델을 검증해보자!

```python
# Visualising the Test set results
plt.scatter(X_test, y_test, color = 'red')
# 테스트를 할때 이미 다 훈련이 되있기 때문에 Train 데이터를 test로 바꿀 필요가 없다.
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Salary vs Experience (Test set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()
```



# Multiple Linear Regression

### data 살펴보기

![image-20191225204651467](/Users/jw/Desktop/study/machineLearning/content/image-20191225204651467.png)

데이터를 살펴보면 5개의 column이 있다. 데이터 분석을 통해서 지역이 매출에 영향을 끼치는지, 마케팅 지출액이 매출에 영향을 끼치는지 알고 싶다. 

![image-20191225210704536](/Users/jw/Desktop/study/machineLearning/content/image-20191225210704536.png)

Multiple Linear Regression 은 Simple Linear Regression과 비슷한데, `y` 는 Dependent variable(DV) 이고, `bn` 과 같은Independent variables(IVs)가 있다. b0는 상수이고, b1은 coeifficient이다.



## Assumptions of a Linear Regression 공부하기

![image-20191225211050282](/Users/jw/Desktop/study/machineLearning/content/image-20191225211050282.png)



## 데이터 살펴보기

![image-20191225211921507](/Users/jw/Desktop/study/machineLearning/content/image-20191225211921507.png)

데이터를 살펴보면 State의 경우 카테고리 데이터이기 때문에 배웠던 대로, column을 분리해야 한다.

![image-20191225212145133](/Users/jw/Desktop/study/machineLearning/content/image-20191225212145133.png)

하지만 이렇게 생성된 2개의 컬럼을 둘다 모델에 적용시키면 안되는데, 그 이유는 New york column만 써도 California를 나타낼 수 있기 때문이다.(1이면 Newyork 0이면 California)

![image-20191225213242636](/Users/jw/Desktop/study/machineLearning/content/image-20191225213242636.png)



## P-value 알아보기

https://www.mathbootcamps.com/what-is-a-p-value/

https://www.wikihow.com/Calculate-P-Value

P-value에 대해서 이해할 필요가 있다!

P-value란 간단히 말하면 모수에서 어떤 sample을 가져왔을 때 그 sample이 얼마나 정확할지 계산하는 것으로 현재 평균보다 모수의 평균이 높을 가능성이 얼마나 되는지에 대한 것이다.

나중에 추후 정리

# Multiple Linear Regression

![image-20191225230427379](/Users/jw/Desktop/study/machineLearning/content/image-20191225230427379.png)

Multiple Linear Regression에서는 어떤 데이터를 살리고, 없앨 것인지 결정해야 한다.

없애야 하는 이유는 2가지가 있다.

1.쓰레기 in, 쓰레기 out 

안좋은 데이터가 들어가면 안좋은 모델이 만들어진다.	

2.설명하고 이해하기가 어렵다!

다른 사람에게 이 모델이 얼마나 정확한지 설명하려면 데이터의 상관 관계를 알려줘야 하는데, 너무 많은 변수를 모델에 적용시키면 분석, 이해하기가 어려워진다.

![image-20191225230513462](/Users/jw/Desktop/study/machineLearning/content/image-20191225230513462.png)



## Multiple Linear Regression(다중 선형 회귀 분석) 모델을 만드는 5가지 방법

단순 선형 회귀(simple Linear Regression)의 목적이

하나의 독립변수만을 가지고 종속 변수를 예측하기 위한 회귀 모형을 만들기 위한 것이였다면

다중 선형 회귀 분석의 목적은 여러 개의 독립 변수(X)들을 가지고 종속 변수(y)를 예측하기 위한 회귀 모형을 만드는 것이다.

여기서 독립 변수들은 종속 변수를 설명한다는 의미에서 설명변수(explanatory variable)라고 부르는 경우가 많다.

그리고 다중 선형 회귀 분석에서도 종속 변수는 반드시 연속형 변수여야 한다.



단순 선형 회귀 분석의 식이 Y=a + b*x 형태로 나타났다면

다중 회귀 분석은 Y = a + b1 * x1 + b2 * x2 + b3 * x3 + ... 의 형태로 나타내 지는데 이 식을 다중 선형 회귀 분석 모형이라고 한다.

이 모형에 포함되는 설명 변수들(x1,x2,...)을 **공변량(covariate)**이라 하고 이 식에서 설명 변수들의 계수(coeffcient)인 b1,b2,b3,b4을 **편회귀 계수(partial regression coefficient)**라고 한다.

예를 들어 b1은 x1외의 다른 설명 변수들이 모두 일정한(조정된 adjusted, 통제된 controled) 상태에서 x1이 1 단위 만큼 변할 때 종속 변수가 얼마나 변하는 가를 알려준다. 즉 설명변수 x1의 종속 변수 y에 대한 영향력을 나타내어 준다.



### 다중 선형 회귀 분석의 장점

다중 선형 회귀 분석은

-  **어떤 설명변수**가 종속변수에 영향을 미치는지를 파악할 수 있게 해준다.
- Standardized Coefficients를 사용하여 각각의 설명변수가 종속변수에 **얼마나 영향을 미치는지** 가늠할 수 있게 해주며
- 이 설명변수들을 사용해 **종속 변수를 가능한 정확히 예측**할 수 있게 해주기 때문에 

매우 유용한 분석 방법이다.

### 다중 선형 회귀 분석의 과정

1. 먼저 회귀 모형에 포함되어질 설명변수들을 선택하고

2. 이 회귀 모형에서 편회귀 계수 추정치(partial regression coefficient)를 구한 뒤, 이렇게 추정된 회귀 모형이

3.  모집단에서 유의한지 아닌지(유의성) -> ANOVA table을 이용한 F 검정 통계량

4.  유의하다면 얼마만큼 실제 값들을 설명해 줄 수 있는지(적합도) -> Adjusted R square(수정된 )

### 다중 선형 회귀 분석의 유의점

다중 선형 회귀 분석은 고려해야 할 설명 변수가 여러 개이기 때문에 가장 적합한 회귀 모형을 만들려고 할 때 복잡한 문제들이 생기고 이를 해결해야 한다.

1. 설명 변수들 중에서 서로 상관 관계가 큰 변수들이 있는 경우(다중공선성 multicollinearity)에 어떤 문제가 발생하며 이를 어떻게 해결할 것인가?
2. 가장 적합한 회귀 모형을 만들기 위해 많은 설명변수들 중에 어떤 설명변수를 회귀 모형에 넣을 것인가?
3. 설명변수가 연속형 변수가 아닌 범주형 변수일 경우는 어떻게 할 것인가? 가변수(dummy variable의 활용)



## 다중공선성(multicollinearity)이란?

### 정의

회귀 모형에 포함될 설명변수들 사이에 서로 밀접한 상관관계가 있어서 다중 선형 회귀 모형에서 이들 각각의 개별 효과를 파악하기 힘들게 되는 현상을 말한다.

### 문제점

설명 변수들 사이에 다중공선성이 있을 때 설명변수 각각에서의 단변수 분석(단순 선형 회귀 분석)에서는 통계적으로 유의하게 나타날지라도 설명 변수들을 모두 고려한 다변수 분석(다중 선형 회귀 분석)에서는 변수들 모두 종속변수와 관련 없는 것으로 나타날 수 있다.

또한 다중 공선성이 있는 설명변수들을 모두 회귀 모형에 포함시킨다면 이로 부터 추정된 편회귀 계수는 신뢰할 수 없게 된다.

### 해결책

가장 확실한 방법은 다중공선성을 가장 크게 유발하는 설명 변수를 회귀 모형에서 제거하는 것이다. 

단, 회귀 모형에 반드시 포함되어야 하는 독립변수를 제거하면 안된다.

![image-20191225231035196](/Users/jw/Desktop/study/machineLearning/content/image-20191225231035196.png)



### 1. All-in

![image-20191225232831662](/Users/jw/Desktop/study/machineLearning/content/image-20191225232831662.png)



### 2. Backward Elimination

![image-20191225235829733](/Users/jw/Desktop/study/machineLearning/content/image-20191225235829733.png)

먼저 모든 설명 변수를 모형에 포함 시킨 후 모형 내에서 가장 덜 중요한 설명변수를 하나씩 제거해 나간다.

(보통 해당 설명변수에 대한 F검정의 p-value가 0.1이상 또는 F검정통계량이 2.71이하 일 때 제거한다)

이러한 절차를 adjusted R square 값에는 유의한 영향을 미치지 않으면서 제거 될 수 있는 설명변수가 없을때까지 반복한다.

## 3.Forward Selection![image-20191228163012095](/Users/jw/Desktop/study/machineLearning/content/image-20191228163012095.png)

아무런 설명변수도 선택하지 않은 상태에서 모형 내에 중요한 설명변수를 하나씩 추가해 나간다. (보통 F 검정의 p-value가 0.05이하 또는 F검정 통계량이 3.84이상 일 때 추가한다)

이러한 절차를 adjusted R Square 값이 유의하게 향상되는 설명변수가 없을때 까지 계속 시행한다.

## 4. 

![image-20191228163920116](/Users/jw/Desktop/study/machineLearning/content/image-20191228163920116.png)

전진 단계 선택과 후진 선택을 결합한 방법이다. 가장 중요한 설명 변수 하나를 전진 선택한다. 그다음 두번째 중요한 설명 변수를 전진 선택한다. 이제 이 두번 째 설명변수가 선택된 후에도 첫 번째 설명 변수가 여전히 중요한 변수인지 평가하기 위해서 후진 제거를 실시한다. 만약 첫번째 설명변수가 여젼히 중요한 설명변수이면 남겨 놓지만 그렇지 않다면 이 설명변수를 제거한다. 이러한 방식으로 설명 변수를 계속해서 선택해 나간다.

## 5. 

![image-20191228163839973](/Users/jw/Desktop/study/machineLearning/content/image-20191228163839973.png)

가장 적합한 회귀 모형을 만들기 위해 종속변수에 영향을 미칠 것 같은 설명 변수들을 경험적, 임상적, 논리적으로 선택하여 설명 변수들의 여러 조합을 만들어 보고 만들어진 각 조합에서 분석을 시행한다.

그 후 각 조합에서 만들어진 모형들의 adjusted R Square를 평가하여 이 값이 가장 큰 모형을 선택한다. 

이 방법은 직접 설명 변수들의 조합을 만들고 각각의 조합마다 분석을 시행하는 수동적인 방법이다.

반면에 후진 제거법, 전진 제거법, 단계적 선택법은 모두 computer intensice한 자동 변수 선택 절차이다.

## 다중선형회귀 모델 코딩

```
# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('50_Startups.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 4].values

# Encoding categorical data
# Encoding the Independent Variable
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X = LabelEncoder()
X[:, 3] = labelencoder_X.fit_transform(X[:, 3])
onehotencoder = OneHotEncoder(categorical_features = [3])
X = onehotencoder.fit_transform(X).toarray()
# Encoding the Dependent Variable
labelencoder_y = LabelEncoder()
y = labelencoder_y.fit_transform(y)

```

