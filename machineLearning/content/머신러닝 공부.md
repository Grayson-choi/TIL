---
typora-copy-images-to: ../content
---

# 데이터 전처리

# 1. 라이브러리 불러오기
가장 중요하고 자주 쓰는 3개의 라이브러리

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
```

# 2. 데이터 불러오기
데이터 불러오기
data.csv를 열어보면 이렇게 생겼다.

![image-20191221172525139](/Users/jw/Library/Application Support/typora-user-images/image-20191221172525139.png)

```python
# 데이터 불러오기
dataset = pd.read_csv('Data.csv')

X = dataset.iloc[:, :-1].values
Y = dataset.iloc[:, 3].values
```



iloc 는 [row, column] 이렇게 데이터를 불러오는건데

`X = dataset.iloc[:, :-1].values` 는 모든 row와 처음부터 -1번째 column까지 불러오라는 뜻이다. 즉 마지막 column빼고 다 들고 오라는 뜻

# 빈 데이터 처리하기

데이터 전처리가 중요한데 데이터를 보면 빈 데이터가 2개 있다.

어떻게 해야할까? 삭제 하는게 나을까? 하지만 중요한 데이터라면 삭제하면 안되는 경우가 있다. 이럴 때 사용하는 것이 sklearn의 Imputer 이다.

```python
from sklearn.preprocessing import Imputer

```



```python
imputer = Imputer(missing_vales = 'NaN', strategy = 'mean', axis = 0)
```

## `Imputer` 설명

`Imputer` 를 누르고 ` command I`를 누르면 라이브러리에 대한 설명이 나온다.

아래는 파라미터 설명이다.

### missing_values

missing_values**integer or "NaN", optional (default="NaN")

없는 데이터를 고른다. 주로 없는 데이터는 NaN으로 나오기 떄문에 이게 기본 값이다.

### strategy 

빈 값을 어떤걸로 채울지 알려주는 거다. mean을 하면 평균이 나오는거고,

중위값, 최빈값으로 설정 가능하다.

- If "mean", then replace missing values using the mean along the axis.
- If "median", then replace missing values using the median along the axis.
- If "most_frequent", then replace missing using the most frequent value along the axis.

### axis

어떤 축을 기준으로 설정할지 알려주는 것이다.

0이면 column을 기준으로, 1이면 row를 기준으로 바꾼다.



The axis along which to impute.

- If axis=0, then impute along columns.
- If axis=1, then impute along rows.



```python
from sklearn.preprocessing import Imputer
imputer = Imputer(missing_vales = 'NaN', strategy = 'mean', axis = 0) #1
imputer = imputer.fit(X[:,1:3]) #2
X[:, 1:3] = imputer.transform(X[:, 1:3]) #3
```

#1 을 통해서 데이터를 바꾸고, #2를 통해서 데이터를 변수 X의 데이터 형태로 맞추고, #3 을 통해서 데이터를 덮어 씌운다.



# Categorical Data

수학식에는 문자가 들어갈 수 없지만, 실제 데이터에는 문자가 존재한다. 따라서 `Encode` 라는 작업을 통해서 데이터를 숫자로 바꿔줘야한다.

```python
from sklearn.preprocessing import LabelEncoder
labelencoder_X = LabelEncoder()
labelencoder_X.fit_transform(X[:, 0])
# 실행 결과
# array([0, 2, 1, 2, 1, 0, 2, 0, 1, 0])
X[:, 0] = labelencoder_X.fit_transform(X[:, 0])

```

1. LabelEncoder 라이브러리를 import 해준다.

2. Labelencder_X 변수에 LabelEncoder 객체를 만들어 준다.(메소드를 사용하기 위해서)

3. `label encoder_X.fit_transform(X[:, 0])` 데이터를 보면 0번째 column이 Country라서 이걸 숫자로 바꿀 것이다.

   ![image-20191222135337464](/Users/jw/Desktop/study/machineLearning/content/image-20191222135337464.png)

4. 코드를 실행하면 `array([0, 2, 1, 2, 1, 0, 2, 0, 1, 0])` 이 잘 출력된다.

5. 이제 결과를 다시 X에 넣어준다.

### Encoding이 가지는 문제가 있다.

지금 텍스트를 바꾸는데 성공했지만, 실제 데이터는 독일, 스페인, 프랑스 3개의 한 카테고리에 묶인 상하 개념이 없는 데이터지만, 숫자료 encoding 한 이후에는 머신러닝 모델의 경우 0이 1보다 크고, 1이 2보다 크다고 생각하기 때문에 이건 정확하지 않은 결과를 만들 수 있다. 따라서 머신러닝 모델이 변경된 숫자 데이터를 하나의 다른 숫자로 바꾸는 작업을 해야한다. 이럴 때 사용하는 것이 dummy variables이다.

이런 것들을 막기 위해서 이제 column의 category 숫자만큼 column을 만든다. 

![image-20191222140918122](/Users/jw/Desktop/study/machineLearning/content/image-20191222140918122.png)

이제 코드를 작성해보자.



```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X = LabelEncoder()
labelencoder_X.fit_transform(X[:, 0])
# 실행 결과
# array([0, 2, 1, 2, 1, 0, 2, 0, 1, 0])
X[:, 0] = labelencoder_X.fit_transform(X[:, 0])
# 어떤 줄을 기준으로 분류할 지 정해준다.
onehotencoder = OneHotEncoder(categorical_features = [0])
# 데이터 형태로 만들어준다.
X = onehotencoder.fit_transform(X).toarray()
```



1. 기존의 코드에 `OneHotEncoder` 라이브러리를 불러오자.
2. `onehotencoder = OneHotEncoder(categorical_features = [0])` 
   카테고리를 몇번째 column 으로 정할지 정해준다.

3. X = onehotencoder.fit_transform(X).toarray() X에 데이터를 넣어준다.

![image-20191222144434768](/Users/jw/Desktop/study/machineLearning/content/image-20191222144434768.png)

기존 데이터와 비교해보면 Country가 3개의 column으로 변경되었다.

1번째는 France 2번째는 Germany 3번째는 Spain이다. 그다음에는 age와 Salary가 들어가 있다.

### Purchased column encoding 하기

Purchased는 2개의 항목 no, yes만 있기 때문에 자동적으로 머신러닝 모델이 데이터간 상하관계가 없다는 것을 깨닫는다. 따라서 LabelEncoder만 해주겠다.

```python
labelencoder_Y = LabelEncoder()
Y = labelencoder_Y.fit_transform(Y)
```



### Updates

문법 변화가 있음 참고용!

in the following tutorial, the first line of code we will type will be:

```
from sklearn.cross_validation import train_test_split
```

However the "cross_validation" name is now deprecated and was replaced by "model_selection" inside the new anaconda versions.

Therefore you might get a warning or even an error if you run this line of code above.

To avoid this, you just need to replace:

```
from sklearn.cross_validation import train_test_split
```

by

```
from sklearn.model_selection import train_test_split
```

and you will get no warning ;)

Hope you are having a great ML journey so far, I'll see you in the next tutorial.



Updates:

***** Also, for those of you who run into Sklearn related deprecations regarding the imputer class you can use the following modifications:

 sklearn.preprocessing.Imputer 클래스는 사이킷런 0.20 버전에서 사용 중지 경고가 발생합니다. 이 클래스는 0.22 버전에서 삭제될 예정입니다. 그 대신 sklearn.impute.SimpleImputer 클래스가 추가되었습니다. SimpleImputer 클래스는 Imputer 클래스와 사용법이 비슷합니다. SimpleImputer를 사용하여 본문처럼 평균을 대체하는 코드는 다음과 같습니다.

OLD (NO)

```python
from sklearn.preprocessing import Imputer
imputer = Imputer(missing_vales = 'NaN', strategy = 'mean', axis = 0) #1
imputer = imputer.fit(X[:,1:3]) #2
X[:, 1:3] = imputer.transform(X[:, 1:3]) #3
```

NEW (YES)

```python
from sklearn.impute import SimpleImputer
missingvalues = SimpleImputer(missing_values = np.nan, strategy = 'mean', verbose = 0)
missingvalues = missingvalues.fit(X[:, 1:3])
X[:, 1:3]=missingvalues.transform(X[:, 1:3])

```



# Splitting the Dataset into the Training set and Test set

데이터가 있을 때 머신러닝을 위해서는 training set 과 test set 2개로 나눠야 한다. 머신 러닝이란 머신들이 데이터를 통해서 모델을 만들면서 학습한다는 것이기 때문에 데이터를 나누는게 중요하다.



## Splitting the dataset into the Training set and Test set

머신러닝 모델을 학습하고 그 결과를 검증하기 위해서는 원래의 데이터를 Training, Validation, Testing의 용도로 나누어 다뤄야 한다. 그렇지 않고 Training에 사용한 데이터를 검증용으로 사용하면 시험문제를 알고 있는 상태에서 공부를 하고 그 지식을 바탕으로 시험을 치루는 꼴이므로 제대로 된 검증이 이루어지지 않기 때문이다. 



딥러닝을 제외하고도 다양한 기계학습과 데이터 분석 툴을 제공하는 scikit-learn 패키지 중 model_selection에는 데이터 분할을 위한 train_test_split 함수가 들어있다.

```python
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
```

`train_test_split()` 의 옵션 설명

**arrays** : 분할시킬 데이터를 입력 *(Python list, Numpy array, Pandas dataframe 등..)*

**test_size** : 테스트 데이터셋의 비율(float)이나 갯수(int) *(default = 0.25)*

**train_size** : 학습 데이터셋의 비율(float)이나 갯수(int) *(default = test_size의 나머지)*

보통 test_size를 정해주면 나머지는 train_size로 정해짐

**random_state** : 데이터 분할시 셔플이 이루어지는데 이를 위한 시드값 *(int나 RandomState로 입력)*

**shuffle** : 셔플여부설정 *(default = True)*

**stratify** : 지정한 Data의 비율을 유지한다. 예를 들어, Label Set인 Y가 25%의 0과 75%의 1로 이루어진 Binary Set일 때, stratify=Y로 설정하면 나누어진 데이터셋들도 0과 1을 각각 25%, 75%로 유지한 채 분할된다.



코드를 실행하면 아래처럼 테스트 데이터에는 2개, 트레인 데이터 8개의 데이터가 들어간다.

![image-20191222164459606](/Users/jw/Desktop/study/machineLearning/content/image-20191222164459606.png)



위 traindata를 보고 훈련을 하고, test 데이터를 보고 모델이 잘 작동하는지 확인한다.

![image-20191222165949551](/Users/jw/Desktop/study/machineLearning/content/image-20191222165949551.png)

테스트에서는 0번 사람이 0번 결과(안산다) 나오고, 1번 사람도 안산다가 나와야 함. 하지만 머신러닝 모델이 hard correlations로 학습했기 때문에 (강한 상관관계?) 모델이 상관관계 파악을 잘 못했을 것 같다. 따라서 좋은 모델을 못만들게 되는데 이게 overfitting이다.

나중에 배운다.

## Feature Scaling

Feature Scaling이란? 

데이터를 보면 이름은 27~50살의 반경을 가지고 있고, 연봉(salary)는 48000~ 83000까지 반경을 가지고 있다.

이러한 범위 격차는 머신러닝 모델에 문제를 일으킬 수 있다.

![image-20191222171115289](/Users/jw/Desktop/study/machineLearning/content/image-20191222171115289.png)



왜냐하면 변수간의 변화를 Euclidean Distance 공식을 이용하는데, 이렇게 되는 경우 salary는 훨씬 큰 값을 갖게 될 것이다.

만약 2번째 Spain과 7번째 France는 salary 차이는 31000 961000000이다

나이 차이는 21 -> 441 이다.

너무 큰 격차가 있기 때문에 같은 범위로 만들어서 -1~1 까지 범위로 변경해야 한다.

![image-20191222171357775](/Users/jw/Desktop/study/machineLearning/content/image-20191222171357775.png)

아래는 정규화 식이다. 수학에 대해서 많이 알 필요는 없지만 기본적인건 알아 두는 것이 좋을 것 같다.

![image-20191222171732312](/Users/jw/Desktop/study/machineLearning/content/image-20191222171732312.png)

### Feature Scaling

```python
# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
```



코드를 실행하면 이런식으로 Feature Scaling이 잘 되었다.

![image-20191222172812653](/Users/jw/Desktop/study/machineLearning/content/image-20191222172812653.png)



## Feature Scaling에 대한 질문

1. dummy variables도 Feature Scaling을 해야하나요? 

   어떤 사람은 필요가 없다고 하고, 어떤 사람은 필요하다고 하는데, 개인적으로는 상황에 따라 다를 것 같다.

2. Feature Scaling을 Y에도 적용해야하나요?

   이번 경우에는 안해도 됩니다. 만약에 데이터가 엄청나게 차이나는 경우 해야겠지만 지금은 0,1 이렇게 나눠져 있기 때문에 안해도 된다.



# Regression

Regression models (both linear and non-linear) are used for predicting a real value, like salary for example. If your independent variable is time, then you are forecasting future values, otherwise your model is predicting present but unknown values. Regression technique vary from Linear Regression to SVR and Random Forests Regression.

In this part, you will understand and learn how to implement the following Machine Learning Regression models:

1. Simple Linear Regression
2. Multiple Linear Regression
3. Polynomial Regression
4. Support Vector for Regression (SVR)
5. Decision Tree Classification
6. Random Forest Classification



### 데이터 살펴보기

데이터를 살펴보면 연봉과 근무연수가 있는데 둘의 관계를 파악하는 것이다.

![image-20191225172801380](/Users/jw/Desktop/study/machineLearning/content/image-20191225172801380.png)



가장 기본적인 simple Linear Regression은 

y = b0 + b1 * x1 이다

y는 Dependent variable (DV)

x는 Independent varable(IV)

b1은 Coeffcient x1의 변화량의 기울기

![image-20191225173656022](/Users/jw/Desktop/study/machineLearning/content/image-20191225173656022.png)

Simple Linear Regression!

그러면 어떻게 이런 best fitting line or Simple Linear Regression 얻는가?

선을 엄청 많이 그어서 가장 차이가 적은 것을 찾는다. 이것이 best fitting line or ordinary least squares method라고 한다.

## 데이터 준비하기



```python
# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Salary_Data.csv')
X = dataset.iloc[:, : -1].values
# 연봉이 1번 index에 있어서 아래와 같이 데이터를 넣음
y = dataset.iloc[:, 1].values

# Splitting the dataset into the Training set and Test set
# 모델이 30개라서 테스트에 10개, train에 20개 넣는다.
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state = 0)
```

테스트 데이터와, train 데이터를 분리하였다. 데이터가 30개여서 10개는 테스트, 20개는 훈련으로 만들었다.

## 훈련 데이터에 fitting Simple Linear Regression 적용시키기

LinearRegression 라이브러리를 사용하여 simple Linear Regression 모델을 만드는 것이다. 보통 regressor라는 변수명을 사용한다.

```python
# Fitting Simple Linear Regression to the Training set
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)

```

모델을 이용하여 Test set의 결과가 정확했는지 확인할 수 있다.

```python
# Predicting the Test set results
y_pred = regressor.predict(X_test)
```

![image-20191225190545334](/Users/jw/Desktop/study/machineLearning/content/image-20191225190545334.png)

왼쪽이 진짜고 오른쪽이 모델을 통해 예측한 예측 값이다. 9번의 경우 예측이 잘 되었고, 7번의 경우 잘 안되었다.



### simple Linear Regression 시각화하기

```python
# Visualising the Training set results
plt.scatter(X_train, y_train, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Salary vs Experience (Training set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()
```

빨간색으로 근속연수와, 연봉을 점 그래프로 나타내고, 선 그래프로 예상 연봉을 만들었다.

![image-20191225191327154](/Users/jw/Desktop/study/machineLearning/content/image-20191225191327154.png)

결과를 보면 데이터들이 선형적으로 잘 나타나있는지 알 수 있다.

그렇다면 과연 새로운 빨간 점(데이터)를 넣으면 어떻게 될까?

테스트 데이터를 넣어서 모델을 검증해보자!



```python
# Visualising the Test set results
plt.scatter(X_test, y_test, color = 'red')
# 테스트를 할때 이미 다 훈련이 되있기 때문에 Train 데이터를 test로 바꿀 필요가 없다.
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Salary vs Experience (Test set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()
```

